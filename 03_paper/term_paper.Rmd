---
title: "Term Paper: Statistical Learning"
author: "Jonathan Berrisch, Timo Rammert"
subtitle: "Subtitle"
type: "Type of Paper"
discipline: "Study Path"
date: "today"
studid: "Matriculation Number"
supervisor: "Prof. Dr. Christoph Hanck"
secondsupervisor: "Prof. Dr. Andreas Behr"
ssemester: "1"
estdegree_emester: "Summer Term 2019"
deadline: "Deadline"
output:
  pdf_document:
    keep_tex: yes
    template: template.tex
    fig_caption: yes
    citation_package: biblatex
    number_sections: true
toc: true
lot: true
lof: true
graphics: true
biblio-title: References
fontsize: 11pt
geometry: lmargin=5cm,rmargin=2.5cm,tmargin=2.5cm,bmargin=2.5cm
biblio-files: references.bib
---

<!-- % Template Version 1.1 -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
```
# Introduction

In this term paper we will search for the best model (in terms of a low RMSE) to predict the variable "Llitre - Weekly sale in log litre" on basis of the friberg gronqvist wine data set. To achieve this we apply different techniques from the branch of statistical learning.

## Data

The data used in this term paper is taken from a wine themed dataset. The variables include among others the name of the wine, the country of origin, the price, the amount (litres), the taste segment and some variables about different reviews.
The variable of interest, which we want to predict, is the variable "Llitre" describing the weekly sale of a specific wine in log litre. It therefore is a transformation of the variable "Litre" which is the weekly sale of a specific wine in litre. The relationship can be written mathematically as 
\begin{equation}
Llitre = \log{Litre}*1000000.
\end{equation}
In order to be able to work with the methods used in this paper we had to omit two variables, namely "time_segm_price" and "artikpr" because those are combinations of other existing variables and therefore would result in the problem of multicollinearity.
Furthermore, we exlude those variables with a ratio of missing values to number of observations larger than $50\%$.
Finally, for some models it is important that the same levels of factor variables (as the levels of these are converted into dummy variables) are included in the used training and test datasets. To achieve these we only do use the intersection of variables included in both sets.

# Analysis

At first a mean regression and a linear regression are calculated and used as a baseline for further methods.
For comparison of the different models, the Root Mean Sqaured Error (RMSE) $$\sqrt{\frac{1}{n}\sum_{i = 1}^{n}\left(y_i-\hat{y}_i\right)^2}$$ is calculated.

```{r Mean Regression, echo = FALSE, fig.cap = "Residuals of the Mean Regression.", message = FALSE}

library(MASS)

load("../00_data/wine_preprocessed.rda")

# Baseline Model: Mean

models <- data.frame(mod = c("Mean", "Basic_lm"), rmse = c(NA, NA))

residuals <- wine$llitre-mean(wine$llitre, na.rm = T) %>% na.omit()
plot(residuals)
models[models$mod == "Mean","rmse"] <- (wine$llitre-mean(wine$llitre, na.rm = T))^2 %>% 
  na.omit() %>%
  sqrt() %>%
  mean()
print(models)
```


```{r ResPlotMR, echo = FALSE, fig.cap = "Histogram and Density of the Residuals of the Mean Regression."}

truehist(residuals)
lines(density(na.omit(residuals)), col = "deeppink", lwd = 2)
```



\pagebreak

## Citations



\pagebreak

## Annotations

### Overview over the RMSEs

```{r RMSE, message = F, echo = F}
library(knitr)
library(kableExtra)

df <- data.frame(
  "Method" = c("Mean-Regression", "Linear Regression", "Forword Stepwise Selection",
               "Backward Stepwise Selection", "Regression Tree", "Pruned Tree",
               "Random Forest"),
  "RMSE" = c(0,0,0,0,0,0,0),
  "No. of Variables" = c(0,0,0,0,0,0,0)
)

kable(df, "latex")


```
