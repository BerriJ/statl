---
title: "A Forecasting Study on Global Wine Sales"
author: "Jonathan Berrisch, Timo Rammert"
subtitle: "Statistical Learning"
type: "Term Paper"
discipline: "VWL M.Sc."
date: "today"
studid: "3071485 | 3030862"
supervisor: "Prof. Dr. Christoph Hanck"
secondsupervisor: "NA"
ssemester: "1"
estdegree_emester: "Summer Term 2019"
deadline: "Aug. 27th 2019"
output:
  pdf_document:
    keep_tex: yes
    template: template.tex
    fig_caption: yes
    citation_package: biblatex
    number_sections: true
toc: true
lot: true
lof: true
graphics: true
biblio-title: References
fontsize: 11pt
geometry: lmargin=5cm,rmargin=2.5cm,tmargin=2.5cm,bmargin=2.5cm
biblio-files: references.bib
---

<!-- % Template Version 1.1 -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(magick)
library(tidyverse)
crop <- function(im, left = 0, top = 0, right = 0, bottom = 0) {
  d <- dim(im[[1]]); w <- d[2]; h <- d[3]
  image_crop(im, glue::glue("{w-left-right}x{h-top-bottom}+{left}+{top}"))
}
```

<!---
Grobe Einteilung der Seiten:

1 - 2 Introduction und Dataset
3 Validation Method (Hier würde ich auf CV und RMSE eingehen)
4-13 Modelle mit den Spezifikationen, den RMSEs etc. Wir haben jetzt folgende Modelle:

# Lineare Modelle:
- Baseline => Mean Regression and Linear Regression
- Selection / Reduction => Lasso, PCR, PLS,
- Nonlinearity => Splines

# Tree based:
- Bagging
- Random Forest
- Boosting

Das sind also grob 6 Modelle und die aktuelle Planung sieht ca. 9 Seiten vor.
Die regel sollte also eine Seite pro Modell sein, und die ausgewählten Modelle dürfen etwas mehr Raum einnehmen.

14-15 Ich denke hier sollten wir uns nicht kurz fassen sondern richtig arbeit investieren. Alles an Infos raus holen was so geht und vieleicht auch noch einmal auf die Vor und Nachteile unserer Modelle eingehen.
-->

# Introduction

This paper presents a forecasting study on global wine sales. This study is based on the friberg gronqvist wine data set which is publicly available and was previously analyzed by [Textcite AER Friberg, Grönqvist]. Recent technological advancements led to an significant increase in techniques that are computationally demanding and are potentially outperforming classical statistical methods, especially in terms of forecasting. Those techniques are usually reffered as statistical- or machine learning methods.

We present various models which are increasingly complex. Those models potentially gain forecasting power while losing interpretability. The applied models range from linear regressions to tree based methods like Random Forests and Boosting. The goal is to develop the best possible model to forecast wine sales in litre.
Although the main goal is an accurate forecast, some of the methods used in this paper can also be used to check whether there is an impact of expert reviews on the sales. This effect of expert opinion on consumer demand is a quite popular topic. E.g. [Textcite Hilger, Rafert, Villas Boas] use a field experiment to study a review-based demand effect using wine score labels in a retail grocery chain. 

The remainder of this paper is structured as follows. Chapter 2 gives an overview about the dataset and the preprocessing. Chapter 3 describes the validation approach. Chapter 4 presents the models and their specification. Chapter 5 provides the results, a conclusion and some ideas for future research.

# Data and Variables

The data set used in this paper contains 145179 observations with 59 variables. The variables include the name of the wine, the country of origin, its price, the amount sold per week in litre, the taste segment and variables related to different reviews among others.
<!---
The variable of interest, which we want to predict, is the variable "litre" describing the weekly sale of a specific wine in log litre. It therefore is a transformation of the variable "Litre" which is the weekly sale of a specific wine in litre. The relationship can be written mathematically as 
\begin{equation}
Llitre = \log{Litre}*1000000.
\end{equation}
-->
We need to omit two variables beforehand, namely "time_segm_price" and "artikpr", because those are combinations of other existing variables. Hence inclusion would lead to the problem of multicollinearity. After omitting those two we have left the following types of variables:

```{r, echo=FALSE}
load("../00_data/output_paper/01_typeof_vec.rda")
knitr::kable(t(typeof_vec), caption = "Frequency of Variable Types")
```

Figure 1 depicts the amount of missing values in each variable. The number of complete cases would be zero without further selection. Therefore we exclude every variable with a ratio of missing values that exceeds $50\%$. In consequence 41416 observations with 45 variables are used for building the forecasting models.

```{r, echo=F, fig.cap= "Share of Missing Values in the Wine Dataset"}
knitr::include_graphics("../00_data/output_paper/02_missings_alt.pdf")
```

Figure XY depicts the distribution of the litre variable which is the dependend variable in our models. One can see that the observations are heavily skewed. The Maximum is at $184200$ litres sold weekly while it's minimum is near zero with only $0.75$ litres sold per week.

```{r, echo = FALSE, fig.cap= "Histogram and Estimated Density of the Litre Variable"}
knitr::include_graphics("../00_data/output_paper/04_hist_litre.pdf")
```

# Validation Approach

Sampling may influence the model selection process because a sample could be in favor of one specific method while another sample could lead to different results. Therefore we validate the results of each method using a 5-fold cross-validation. This reduces the influence of sampling when building a training and test set while it's computationally feasible. 

In order to compute a prediction it's necessary that the test set includes, at least all, features of the training set. Due to the high number of levels in the countries and name variable this was not always the case. Therefore we are using only the intersection of training and test features to be included in the estimation.

# Analysis

## Mean Regression and Linear Regression

```{r, echo = F}
# Load Baseline Summary
load(file = "../00_data/output_paper/03_baseline.rda")
```

At first a mean regression and a linear regression are calculated. Those models are representing the baseline.
For comparison of the different models, the Root Mean Sqaured Error (RMSE) $$\sqrt{\frac{1}{n}\sum_{i = 1}^{n}\left(y_i-\hat{y}_i\right)^2}$$ is calculated.

The mean regression yields an average RMSE of $\approx `r paste(colMeans(df)[2] %>% round())`$ litre sold per week. The linear regression where all variables are included yields an average RMSE of $\approx `r colMeans(df)[1] %>% round()`$. The latter results is probably influenced by overfitting. To cope with this problem we are using variable selection and dimension reduction methods which are discussed in the proceeding sections. 

```{r Mean Regression, echo = FALSE, fig.cap = "Residuals of the Mean Regression.", message = FALSE}

# library(MASS)
# 
# load("../00_data/wine_preprocessed.rda")
# 
# # Baseline Model: Mean
# 
# models <- data.frame(mod = c("Mean", "Basic_lm"), rmse = c(NA, NA))
# 
# residuals <- wine$llitre-mean(wine$llitre, na.rm = T) %>% na.omit()
# # plot(residuals)
# models[models$mod == "Mean","rmse"] <- (wine$llitre-mean(wine$llitre, na.rm = T))^2 %>% 
#   na.omit() %>%
#   sqrt() %>%
#   mean()
# print(models)
```

```{r ResPlotMR, echo = FALSE, fig.cap = "Histogram and Density of the Residuals of the Mean Regression."}

# truehist(residuals)
# lines(density(na.omit(residuals)), col = "deeppink", lwd = 2)
```

## Lasso

```{r, echo = F}
# Load Baseline Summary
load(file = "../00_data/output_paper/05_lasso.rda")
```

The Least Absolute Shrinkage and Selection Operator (Lasso) is a method combining shrinkage of the coefficent estimates to do variable selection. It fits a linear model that is constrained by a penalty term, i.e. the lasso coefficients minimize:

$$
\sum_{i=1}^{n}(y_i - \beta_0 - \sum_{j=1}^{p}\beta_jx_{ij})+\lambda\sum_{j=1}^{p}|\beta_j|.
$$

Figure XY depicts the relation between lambda and the coefficients. The crossvalidated log lambda, which minimizes the test error is $\approx `r df[3]`$. At that level a total of $\approx `r floor(df[4])`$ out of $713$ coefficients are nonzero\footnote{$\approx `r df[4]`$ to be exact, but we couldn't resists to round down.}. The other coefficients are exactly zero due to the sparsitiy property of the lasso estimator. Furthermore, the plot includes abbreviated variable names of the 10 biggest coefficients. All of those coefficients are dummys for specif*ic wine names.
The average RMSE of the lasso approach is $\approx `r df[1]`$. This is a slight improvement above the linear baseline model.
The lasso slightly reduce the feature set while also reducing the RMSE. This is evidence which supports our prior assimption that the linear model was exposed to the problem of overfitting.
Furthermore, the coefficients of the lasso model show that the name variable is the most important variable to explain the sold litre per week. It is followed by the Vintage, the Region, the taste segment and the Country of the wine. The least important variables are the review variables which are not present in the 400 biggest coefficients.

```{r, echo = FALSE, fig.cap= "Relation of Coefficients and Shrinkage"}
image_read("../00_data/output_paper/06_lasso_vars.png") %>%
  crop(bottom = 50)
```

## PCR and PLS

```{r, echo = F}
# Load PCR_PLS Summary
load(file = "../00_data/output_paper/07_pca_pls.rda")
cm_df <- df %>% colMeans() %>% round(2)
```

While lasso is a technique for variable selection, PCR and PLS are methods that reduce the dimension of the feature set. Thus, the least squares estimation is performed using a transformation of the explanatory variables. Thereby, PLS includes the dependend variable when computing the new feature set while PCR builds the principal components independend of the dependend variable.

Figure XY shows the first two principal components from a principal components analysis. The first two principal components represent roughly 3\% of the total variance. This is a sign that the PCA doesn't work well in our data which is might be due to the large amount of dummy variables. 

```{r, echo = FALSE, fig.cap= "Principal Component One and Two", message=FALSE}
image_read("../00_data/output_paper/07_pca.png") %>%
  crop(bottom = 120)
```

The results confirms the expectation. PCR has a cross validated RMSE of $\approx `r cm_df[1]`$ while PLS leads to an RMSE of $\approx `r cm_df[2]`$. While $\approx `r cm_df[4]`$ principal component where included in the PCR, only $\approx `r cm_df[3]`$ components where used in the PLS. Thus PLS significantly reduced the dimension of the featureset. While sacrificing interpretability of the results, it leads to a higher RMSE.

## Splines

The previous methods assumed linear relationships between the independend variables and the dependend variable. Considering the amount of dummy variables in the featureset the usage of nonlinear methods is quite limited. 
However, to recognize potential nonlinear relationships we add natural splines to the $year$, $price$, and $rprice_litre$ variable. Thos variables indicate the year in which the wine was distributed, the price and the real price with Jan 2004 as base respectively. Each natural spline was allowed to have up to 20 knots which is sufficient to estimate complex linear relationships.
The respective RMSE's depending on the knots are presented in Figure XY.
Splines are able to improve the prediction a little which indicates that at least some nonlinear relationship is present. However, like the methods before, splines reduce the interpretability of the coefficients. In this case, this tradeoff isn't offset enough by the gain in precision.
Furthermore, Figure XY shows, that the RMSE's depend to some extend on the Fold which was used for Crossvalidation. This approves our theoretically founded motivation to use crossvalidation.

```{r, echo = FALSE, fig.cap= "Dependency Between Knots and RMSE"}
knitr::include_graphics("../00_data/output_paper/08_splines.pdf")
```

## Decision tree methods

Tree-based methods utilze decision rules to split the feature space into a number of different regions. ``Since the set of splitting rules used to segment the predictor space can be summarized in a tree, these type of approaches are known as decision tree methods'' [Textcite Hastie, p. 303]. The base of tree-based methods are regression trees
For improvement of the predictive power of these methods, while losing interpretability, one can combine different regression trees for a single prediction. Those approaches, including bagging, random forests and boosting, are described in more detail when being applied.

### Regression Tree

For growing a regression tree `The algorithm needs to automatically decide on the splitting variables and split points [...] <!--- , and also what topology (shape)---> the tree should have'[@Hastie2013, p.349]. The tree is grown when a splitting is found that minimizes the sum of squared residuals. The splitting is done with a greedy algorithm, i.e., at first all the data is split into just two groups while the search of the splitting variable and split point includes all possible variables and points [cf. @Hastie2013, p.349]. After the data is split this process is repeated for the now obtained two splitted regions until the tree is large enough that the nodes reach a set minimum size.

```{r tree, echo = FALSE, fig.cap = "Example of a Regression Tree."}
knitr::include_graphics("../00_data/output_paper/09_tree.pdf")
```

Figure XY shows that only a few of the available variables are used. These are namely $ms_segm$, $segmred$, $price$, and $period$ and split the dataset into $10$ parts, i.e. the tree has got $10$ terminal nodes. The figure does also tell us what share of the data can be sorted to each node, as well as the mean price of the allocated observations. [[[ The regression trees build in the other four folds of the cross-validation are almost exactly the same as the shown tree. ]]] The selected variables indicate that according to this method the market share, the color of the wine, the price as well as the period. Concerning the graph and the variable $segmred$ it is important to keep in mind that this is a dummy variable meaning that a value $< 0.5$ (the left track) stands for a wine that isn't red. Conversely the right track stands for red wines.

As regression trees may tend to grow quite large, a possible way to get smaller trees is pruning back the grown larger tree. The goal of pruning is `to select a subtree that leads to the lowest test error rate [which] we can estimate [...] using cross-validation or the validation set approach' [@James2014, p. 308]. This is done by *cost complexity pruning*.

```{r prune, echo = FALSE, fig.cap = "Example of a Pruned Tree."}
knitr::include_graphics("../00_data/output_paper/09_tree_pruned.pdf")
```

```{r, echo = F}
# Load Tree Summary
load(file = "../00_data/output_paper/09_tree.rda")
cm_df <- df %>% colMeans() %>% round(2)
```

While the pruned tree might be easier to interpret, it does also come with a higher rmse.
The mean cross-validated rmse of the pruned trees is ca. `r cm_df[2]` while the mean rmse of the normal regression trees is about `r cm_df[1]`.


### Random Forest

Random forests are in basic a combination of different regression trees. The name random forest arises from the fact that at each split of the tree a set number of randomly chosen variables are considered for the next splitting. This results in a different tree for each iteration off the random forest and thus by averaging leads to a lower variance <!--- in terms of forecasting error --> than the single trees. This is especially the case 

```{r, echo = FALSE, fig.cap= "Dependency between RMSE, the Number of Trees and the Number of Variables Included at each Split", fig.scap= "RMSE's of the Random Forest for Different Parameters", fig.align= 'center'}
knitr::include_graphics("../00_data/output_paper/10_rf_plot.pdf")
```

### Bagging

Bagging is a special version of a random forest in which at each split all possible variables are taken in consideration for selection. In contrast to random forests this may lead to very similar trees at each iteration which thus ca be highly correlated.

### Boosting

```{r, echo = FALSE, fig.cap= "Dependency between RMSE, Lambda, the Depths and the Number of Trees that are Grown", fig.scap= "RMSE's of the Boosting Model for Different Parameters", fig.align= 'center'}
knitr::include_graphics("../00_data/output_paper/11_boosting_plot.pdf")
```

In contrast to random forests boosting grows tree sequentially meaning that for every new tree information from already grown trees is used [@James2014, p.322]. For boosting rather small regression trees are used to improve the boosting tree slowly. This is achieved by taking the residuals from the current boosting tree, fitting a (small) tree to these residuals and then fiiting this tree into the boosting tree in areas where the performance of the boosting tree could be improved [@James2014, p.322].


# Conclusion

\pagebreak

## Citations



\pagebreak

## Annotations

### Overview over the RMSEs

```{r RMSE, message = F, echo = F}
library(knitr)
library(kableExtra)

df <- data.frame(
  "Method" = c("Mean-Regression", "Linear Regression", "Forword Stepwise Selection",
               "Backward Stepwise Selection", "Regression Tree", "Pruned Tree",
               "Random Forest"),
  "RMSE" = c(0,0,0,0,0,0,0),
  "No. of Variables" = c(0,0,0,0,0,0,0)
)

kable(df, "latex")


```
