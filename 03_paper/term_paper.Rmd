---
title: "A Forecasting Study on Global Wine Sales"
author: "Jonathan Berrisch, Timo Rammert"
subtitle: "Statistical Learning"
type: "Term Paper"
discipline: "VWL M.Sc."
date: "today"
studid: "3071485 | 3030862"
supervisor: "Prof. Dr. Christoph Hanck"
secondsupervisor: "NA"
ssemester: "1"
estdegree_emester: "Summer Term 2019"
deadline: "Aug. 27th 2019"
output:
  pdf_document:
    keep_tex: yes
    template: template.tex
    fig_caption: yes
    citation_package: biblatex
    number_sections: true
toc: true
lot: true
lof: true
graphics: true
biblio-title: References
fontsize: 11pt
geometry: lmargin=5cm,rmargin=2.5cm,tmargin=2.5cm,bmargin=2.5cm
biblio-files: references.bib
---

<!-- % Template Version 1.1 -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
```

<!---
Grobe Einteilung der Seiten:

1 - 2 Introduction und Dataset
3 Validation Method (Hier würde ich auf CV und RMSE eingehen)
4-13 Modelle mit den Spezifikationen, den RMSEs etc. Wir haben jetzt folgende Modelle:

# Lineare Modelle:
- Baseline => Mean Regression and Linear Regression
- Selection / Reduction => Lasso, PCR, PLS,
- Nonlinearity => Splines

# Tree based:
- Bagging
- Random Forest
- Boosting

Das sind also grob 6 Modelle und die aktuelle Planung sieht ca. 9 Seiten vor.
Die regel sollte also eine Seite pro Modell sein, und die ausgewählten Modelle dürfen etwas mehr Raum einnehmen.

14-15 Ich denke hier sollten wir uns nicht kurz fassen sondern richtig arbeit investieren. Alles an Infos raus holen was so geht und vieleicht auch noch einmal auf die Vor und Nachteile unserer Modelle eingehen.
-->

# Introduction

This paper presents a forecasting study on global wine sales. This study is based on the friberg gronqvist wine data set which is publicly available and was previously analyzed by [Textcite AER Friberg, Grönqvist]. Recent technological advancements led to an significant increase in techniques that are computationally demanding and are potentially outperforming classical statistical methods, especially in terms of forecasting. Those techniques are usually reffered as statistical- or machine learning methods.

We present various models which are increasingly complex. Those models potentially gain forecasting power while losing interpretability. The applied models range from linear regressions to tree based methods like Random Forests and Boosting. The goal is to develop the best possible model to forecast wine sales in litre.
Although the main goal is an accurate forecast, some of the methods used in this paper can also be used to check whether there is an impact of expert reviews on the sales. This effect of expert opinion on consumer demand is a quite popular topic. E.g. [Textcite Hilger, Rafert, Villas Boas] use a field experiment to study a review-based demand effect using wine score labels in a retail grocery chain. 

The remainder of this paper is structured as follows. Chapter 2 gives an overview about the dataset and the preprocessing. Chapter 3 describes the validation approach. Chapter 4 presents the models and their specification. Chapter 5 provides the results, a conclusion and some ideas for future research.

# Data and Variables

The datset contains 145179 observations with 59 variables. The variables include the name of the wine, the country of origin, its price, the amount sold per week in litre, the taste segment and variables related to different reviews among others.
<!---
The variable of interest, which we want to predict, is the variable "litre" describing the weekly sale of a specific wine in log litre. It therefore is a transformation of the variable "Litre" which is the weekly sale of a specific wine in litre. The relationship can be written mathematically as 
\begin{equation}
Llitre = \log{Litre}*1000000.
\end{equation}
-->
We need to omit two variables beforehand, namely "time_segm_price" and "artikpr", because those are combinations of other existing variables. Hence inclusion would lead to the problem of multicollinearity in some models. After omitting those two we have left the following types of variables:

```{r, echo=FALSE}
load("../00_data/output_paper/01_typeof_vec.rda")
knitr::kable(t(typeof_vec))
```

Figure 1 depicts the amount of missing values in each variable. The number of complete cases would be zero without further selection. Therefore we exclude every variable with a ratio of missing values that exceeds $50\%$. In consequence 41416 observations with 45 variables are used for building the forecasting models.

```{r, echo=F}
knitr::include_graphics("../00_data/output_paper/02_missings.pdf")
```

<!---
Finally, for some models it is important that the same levels of factor variables (as the levels of these are converted into dummy variables) are included in the used training and test datasets. To achieve these we only do use the intersection of variables included in both sets.
-->

# Validation Approach

Here we go describing our approach to evaluate overall model performance and tune parameters. => CV and loops for hyperparameters... We should make it very clear that every rmse that is presented is the results of a 5 fold cross validation!

For validation of the results obtained in this paper a 5-fold cross-validation of each method is performed. This also includes a validation and thus a more robust estimate of the criterion for comparison of the models. When cross-validating it is important to make sure that the exact same variables are part of either the training and test set.

# Analysis

## Mean Regression and Linear Regression

At first a mean regression and a linear regression are calculated and used as a baseline for further methods.
For comparison of the different models, the Root Mean Sqaured Error (RMSE) $$\sqrt{\frac{1}{n}\sum_{i = 1}^{n}\left(y_i-\hat{y}_i\right)^2}$$ is calculated.

```{r Mean Regression, echo = FALSE, fig.cap = "Residuals of the Mean Regression.", message = FALSE}

library(MASS)

load("../00_data/wine_preprocessed.rda")

# Baseline Model: Mean

models <- data.frame(mod = c("Mean", "Basic_lm"), rmse = c(NA, NA))

residuals <- wine$llitre-mean(wine$llitre, na.rm = T) %>% na.omit()
# plot(residuals)
models[models$mod == "Mean","rmse"] <- (wine$llitre-mean(wine$llitre, na.rm = T))^2 %>% 
  na.omit() %>%
  sqrt() %>%
  mean()
print(models)
```


```{r ResPlotMR, echo = FALSE, fig.cap = "Histogram and Density of the Residuals of the Mean Regression."}

truehist(residuals)
lines(density(na.omit(residuals)), col = "deeppink", lwd = 2)
```




## Lasso

The Least Absolute Shrinkage and Selection Operator (Lasso) is a method combining shrinkage of the coefficent estimates as well as variable selection. Basically it fits a linear model but is constrained by a penalty term, i.e. the lasso coefficients minimize

$$
\sum_{i=1}^{n}(y_i - \beta_0 - \sum_{j=1}^{p}\beta_jx_{ij})+\lambda\sum_{j=1}^{p}|\beta_j|.
$$

```{r Lasso}
load("../02_analysis/cv/lasso/models_df_20190816_1222_1.rda")
```

## PLS and PCR

## Splines

## Regression Trees and Random Forests

### Regression Tree

### Pruned Tree

### Bagging

### Random Forest

### Boosting

# Conclusion

\pagebreak

## Citations



\pagebreak

## Annotations

### Overview over the RMSEs

```{r RMSE, message = F, echo = F}
library(knitr)
library(kableExtra)

df <- data.frame(
  "Method" = c("Mean-Regression", "Linear Regression", "Forword Stepwise Selection",
               "Backward Stepwise Selection", "Regression Tree", "Pruned Tree",
               "Random Forest"),
  "RMSE" = c(0,0,0,0,0,0,0),
  "No. of Variables" = c(0,0,0,0,0,0,0)
)

kable(df, "latex")


```
