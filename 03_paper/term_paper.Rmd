---
title: "A Forecasting Study on Global Wine Sales"
author: "Jonathan Berrisch, Timo Rammert"
subtitle: "Statistical Learning"
type: "Term Paper"
discipline: "VWL M.Sc."
date: "today"
studid: "3071485 | 3030862"
supervisor: "Dr. Thomas Deckers"
secondsupervisor: "NA"
ssemester: "1"
estdegree_emester: "Summer Term 2019"
deadline: "Aug. 27th 2019"
output:
  pdf_document:
    keep_tex: yes
    template: template.tex
    fig_caption: yes
    citation_package: biblatex
    number_sections: true
toc: true
lot: true
lof: true
graphics: true
biblio-title: References
fontsize: 11pt
geometry: lmargin=5cm,rmargin=2.5cm,tmargin=2.5cm,bmargin=2.5cm
biblio-files: references.bib
---

<!-- % Template Version 1.1 -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(magick)
library(tidyverse)
library(stargazer)
crop <- function(im, left = 0, top = 0, right = 0, bottom = 0) {
  d <- dim(im[[1]]); w <- d[2]; h <- d[3]
  image_crop(im, glue::glue("{w-left-right}x{h-top-bottom}+{left}+{top}"))
}
```

<!---
Grobe Einteilung der Seiten:

1 - 2 Introduction und Dataset
3 Validation Method (Hier w체rde ich auf CV und RMSE eingehen)
4-13 Modelle mit den Spezifikationen, den RMSEs etc. Wir haben jetzt folgende Modelle:

# Lineare Modelle:
- Baseline => Mean Regression and Linear Regression
- Selection / Reduction => Lasso, PCR, PLS,
- Nonlinearity => Splines

# Tree based:
- Bagging
- Random Forest
- Boosting

Das sind also grob 6 Modelle und die aktuelle Planung sieht ca. 9 Seiten vor.
Die regel sollte also eine Seite pro Modell sein, und die ausgew채hlten Modelle d체rfen etwas mehr Raum einnehmen.

14-15 Ich denke hier sollten wir uns nicht kurz fassen sondern richtig arbeit investieren. Alles an Infos raus holen was so geht und vieleicht auch noch einmal auf die Vor und Nachteile unserer Modelle eingehen.
-->

# Introduction

This paper presents a forecasting study on global wine sales. This study is based on the friberg gronqvist wine data set which is publicly available and was previously analyzed by \textcite[][p. 193f.]{Friberg2012}. Recent technological advancements led to a significant increase in statistical methods that are computationally demanding while potentially outperforming classical statistical methods, especially in terms of forecasting. Those techniques are usually reffered as statistical- or machine learning methods.

We present various models which are increasingly complex. Those models potentially gain forecasting power while losing interpretability. The applied models range from linear regressions to tree based methods like random forests and boosting. The goal is to develop the best possible model to forecast weekly wine sales in litre in out of sample data.

Although the main goal is an accurate forecast, some of the methods used in this paper can also be used to quantify the impact of expert reviews on the sales, if there is any. The effect of expert opinions on consumer demand was analyzed in preceeding research. Notably, \textcite[][p.1289]{Hilger2011} used a field experiment to study a review-based demand effect using wine score labels in a retail grocery chain. They find that providing information based on expert opinions increases the sales; this effect does depend on the score a wine has gotten. \textcite[][p. 293]{Ashenfelter2013} analyze, wether expert reviews influence the prices of wine. They conclude, that expert reviews contain public as well as private information. They only validate an influence on the highest rated wines. Furthermore, \textcite[][p. 182-183]{Bicknell2012} delivered research concerning the market for wine from new zealand. They validate a significant regional influence on wine prices. This influence is substantially lower for wine which is designated for the export market.

The remainder of this paper is structured as follows. Chapter 2 gives an overview about the dataset and the preprocessing. Chapter 3 describes the validation approach. Chapter 4 presents the models and their specification. Chapter 5 provides the results, a conclusion and some ideas for future research.

# Data and Variables

The data set used in this paper contains 145179 observations with 59 variables. The variables include the name of the wine, the country of origin, its price, the amount sold per week in litre, the taste segment and variables related to different reviews among others.
<!---
The variable of interest, which we want to predict, is the variable "litre" describing the weekly sale of a specific wine in log litre. It therefore is a transformation of the variable "Litre" which is the weekly sale of a specific wine in litre. The relationship can be written mathematically as
\begin{equation}
Llitre = \log{Litre}*1000000.
\end{equation}
-->
We need to omit two variables beforehand, namely "time_segm_price" and "artikpr", because those are combinations of other existing variables. Hence inclusion would lead to the problem of multicollinearity. After omitting those two we have left the following types of variables:

```{r, echo=FALSE}
load("../00_data/output_paper/01_typeof_vec.rda")
knitr::kable(t(typeof_vec), caption = "Frequency of Variable Types")
```

Figure 1 depicts the amount of missing values in each variable. The number of complete cases would be zero without further selection. Therefore we exclude every variable with a ratio of missing values that exceeds $50\%$. In consequence 41416 observations with 45 variables are used for building the forecasting models.

```{r, echo=F, fig.cap= "Share of Missing Values in the Wine Dataset"}
knitr::include_graphics("../00_data/output_paper/02_missings_alt.pdf")
```

Figure XY depicts the distribution of the litre variable which is the dependend variable in our models. One can see that the observations are heavily skewed. The Maximum is at $184200$ litres sold weekly while it's minimum is near zero with only $0.75$ litres sold per week.

```{r, echo = FALSE, fig.cap= "Histogram and Estimated Density of the Litre Variable"}
knitr::include_graphics("../00_data/output_paper/04_hist_litre.pdf")
```

# Validation Approach

Sampling may influence the model selection process because a sample could be in favor of one specific method while another sample could lead to different results. Therefore we validate the results of each method using a 5-fold cross-validation. This reduces the influence of sampling when building a training and test set while it's computationally feasible.

In order to compute a prediction it is necessary that the test set includes, at least, all features of the training set. Due to the high number of levels in the countries and name variable this was not always the case. Therefore, we are using only the intersection of training and test features to be included in the estimation.

# Analysis

## Mean Regression and Linear Regression

```{r, echo = F}
# Load Baseline Summary
load(file = "../00_data/output_paper/03_baseline.rda")
```

At first a mean regression and a linear regression are calculated. Those models are representing the baseline.
For comparison of the different models, the Root Mean Sqaured Error (RMSE) $$\sqrt{\frac{1}{n}\sum_{i = 1}^{n}\left(y_i-\hat{y}_i\right)^2}$$ is calculated.

The mean regression yields an average RMSE of $\approx `r paste(colMeans(df)[2] %>% round())`$ litre sold per week. The linear regression where all variables are included yields an average RMSE of $\approx `r colMeans(df)[1] %>% round()`$. The latter result is probably influenced by overfitting. To cope with this problem, we are using variable selection and dimension reduction methods which are discussed in the proceeding sections.

```{r Mean Regression, echo = FALSE, fig.cap = "Residuals of the Mean Regression.", message = FALSE}

# library(MASS)
#
# load("../00_data/wine_preprocessed.rda")
#
# # Baseline Model: Mean
#
# models <- data.frame(mod = c("Mean", "Basic_lm"), rmse = c(NA, NA))
#
# residuals <- wine$llitre-mean(wine$llitre, na.rm = T) %>% na.omit()
# # plot(residuals)
# models[models$mod == "Mean","rmse"] <- (wine$llitre-mean(wine$llitre, na.rm = T))^2 %>%
#   na.omit() %>%
#   sqrt() %>%
#   mean()
# print(models)
```

```{r ResPlotMR, echo = FALSE, fig.cap = "Histogram and Density of the Residuals of the Mean Regression."}

# truehist(residuals)
# lines(density(na.omit(residuals)), col = "deeppink", lwd = 2)
```

## Lasso

```{r, echo = F}
# Load Baseline Summary
load(file = "../00_data/output_paper/05_lasso.rda")
```

The Least Absolute Shrinkage and Selection Operator (Lasso) is a method combining linear regression and shrinkage of the coefficent estimates to do variable selection. It fits a linear model that is constrained by a penalty term, i.e., the lasso coefficients minimize:

$$
\sum_{i=1}^{n}(y_i - \beta_0 - \sum_{j=1}^{p}\beta_jx_{ij})+\lambda\sum_{j=1}^{p}|\beta_j|.
$$

Figure XY depicts the relation between lambda and the coefficients. The crossvalidated log lambda, which minimizes the test error is $\approx `r df[3]`$. At that level a total of $\approx `r floor(df[4])`$ out of $713$ coefficients are nonzero\footnote{$\approx `r df[4]`$ to be exact, but we couldn't resists to round down.}. The other coefficients are exactly zero due to the sparsitiy property of the lasso estimator. Furthermore, the plot includes abbreviated variable names of the 10 biggest coefficients. All of those coefficients are dummys for specif*ic wine names.
The average RMSE of the lasso approach is $\approx `r df[1]`$. This is a slight improvement compared to the linear baseline model.
The lasso slightly reduces the feature set while also reducing the RMSE. This is evidence supporting our prior assimption that the linear model was exposed to the problem of overfitting.
Furthermore, the coefficients of the lasso model show that the name variable is the most important variable to explain the sold litre per week. It is followed by the Vintage, the Region, the taste segment and the Country of the wine. The least important variables are the review variables which are not present in the 400 biggest coefficients.

```{r, echo = FALSE, fig.cap= "Relation of Coefficients and Shrinkage"}
image_read("../00_data/output_paper/06_lasso_vars.png") %>%
  crop(bottom = 50)
```

## PCR and PLS

```{r, echo = F}
# Load PCR_PLS Summary
load(file = "../00_data/output_paper/07_pca_pls.rda")
cm_df <- df %>% colMeans() %>% round(2)
```

While lasso is a technique for variable selection, PCR (principal components regression) and PLS (partial least squares) are methods that reduce the dimension of the feature set. Thus, the least squares estimation is performed using a transformation of the explanatory variables. Thereby, PLS includes the dependent variable when computing the new feature set while PCR builds the principal components independent of the dependent variable.

Figure XY shows the first two principal components from a principal components analysis. The first two principal components represent roughly 3\% of the total variance. This is a sign that the PCA doesn't work well in our data which might be due to the large amount of dummy variables.

```{r, echo = FALSE, fig.cap= "Principal Component One and Two", message=FALSE}
image_read("../00_data/output_paper/07_pca.png") %>%
  crop(bottom = 120)
```

The results confirm the expectation. PCR has a cross-validated RMSE of $\approx `r cm_df[1]`$ while PLS leads to an RMSE of $\approx `r cm_df[2]`$. While $\approx `r cm_df[4]`$ principal component where included in the PCR, only $\approx `r cm_df[3]`$ components where used in the PLS. Thus PLS significantly reduced the dimension of the featureset. While sacrificing interpretability of the results, it leads to a higher RMSE.

## Splines

The previous methods assumed linear relationships between the independent variables and the dependent variable. Considering the amount of dummy variables in the feature set the usage of nonlinear methods is quite limited.
However, to recognize potential nonlinear relationships we add natural splines to the $year$, $price$, and $rprice\_litre$ variable. Those variables indicate the year in which the wine was distributed, the price and the real price with Jan 2004 as base respectively. Each natural spline was allowed to have up to 20 knots which is sufficient to estimate complex linear relationships.
The respective RMSEs depending on the knots are presented in Figure XY.
Splines are able to improve the prediction a little which indicates that at least some nonlinear relationship is present. However, like the methods before, splines reduce the interpretability of the coefficients. In this case, this tradeoff isn't offset enough by the gain in precision.
Furthermore, Figure \ref{fig:splines} shows, that the RMSEs depend to some extend on the Fold which was used for cross-validation. This approves our theoretically founded motivation to use cross-validation.

```{r splines, echo = FALSE, fig.cap= "\\label{fig:splines}Regression with Splines: Dependency Between Knots and RMSE", fig.align= T, fig.scap= "RMSE Values of Different Spline Models"}
knitr::include_graphics("../00_data/output_paper/08_splines.pdf")
```

## Decision tree methods

Tree-based methods utilze decision rules to split the feature space into a number of different regions. ``Since the set of splitting rules used to segment the predictor space can be summarized in a tree, these type of approaches are known as decision tree methods'' [Textcite Hastie, p. 303]. The base of tree-based methods are regression trees
For improvement of the predictive power of these methods, while losing interpretability, one can combine different regression trees for a single prediction. Those approaches, including bagging, random forests and boosting, are described in more detail when being applied.

### Regression Tree

For growing a regression tree `the algorithm needs to automatically decide on the splitting variables and split points [...] <!--- , and also what topology (shape)---> the tree should have'[@Hastie2013, p.349]. The tree is grown when a splitting is found that minimizes the sum of squared residuals. The splitting is done with a greedy algorithm, i.e., at first all the data is split into just two groups while the search of the splitting variable and split point includes all possible variables and points [cf. @Hastie2013, p.349]. After the data is split this process is repeated for the now obtained two splitted regions until the tree is large enough that the nodes reach a set minimum size.

```{r tree, echo = FALSE, fig.cap = "\\label{fig:tree}Example of a Regression Tree."}
knitr::include_graphics("../00_data/output_paper/09_tree.pdf")
```

Figure \ref{fig:tree} shows that only a few of the available variables are used. These are namely $ms\_segm$, $segmred$, $price$, and $period$ and split the dataset into $10$ parts, i.e., the tree has got $10$ terminal nodes. The figure does also tell us what share of the data can be sorted to each node, as well as the mean price of the allocated observations. [[[ The regression trees build in the other four folds of the cross-validation are almost exactly the same as the shown tree. ]]] The selected variables indicate that according to this method the market share, the color of the wine, the price as well as the period. Concerning the graph and the variable $segmred$ it is important to keep in mind that this is a dummy variable meaning that a value $< 0.5$ (the left track) stands for a wine that isn't red. Conversely the right track stands for red wines.

As regression trees may tend to grow quite large, a possible way to get smaller trees is pruning back the grown larger tree. The goal of pruning is `to select a subtree that leads to the lowest test error rate [which] we can estimate [...] using cross-validation or the validation set approach' \autocite[][p. 308]{James2014}, p. 308]. This is done by *cost complexity pruning*.

```{r prune, echo = FALSE, fig.cap = "\\label{fig:tree_pruned}Example of a Pruned Tree."}
knitr::include_graphics("../00_data/output_paper/09_tree_pruned.pdf")
```

```{r, echo = F}
# Load Tree Summary
load(file = "../00_data/output_paper/09_tree.rda")
cm_df <- df %>% colMeans() %>% round(2)
```

The pruned tree (see Figure \ref{fig:tree_pruned}) does only utilze the dummy whether the wine is a red wine and the mean market share within color during weeks the wine is distributed. $ms\_segm$ is used twice and thus the pruned tree splits the data set into four terinal nodes. While the pruned tree thus might be easier to interpret, it does also come with a higher RMSE.
The mean cross-validated RMSE of the pruned trees is ca. `r cm_df[2]`, while the mean RMSE of the normal regression trees is about `r cm_df[1]`.

### Bootstrap Aggregation

Bootstrap aggregation (Bagging) is a general method of repeatedly taking bootstrap samples from the dataset, estimating a model for every sample and averaging the predictions of every model afterwards.
The bagging algorithm can also be used to improve decision tree models. For this, a bootstrap sample is drawn before growing the tree. This process is repeated several times to grow multiple trees. Finally, the predictions of the trees are averaged.
By bootstrapping bagging brings down the variance and thus, adresses the main problem of single regression trees which are very dependent on the actual sample.

Figure \ref{fig:rmsebag} shows the RMSE values of different bagging models. The bootstrap sample size was chosen to be two thirds of the total number of observations. One can see that even small models with down to 15 Trees significantly outperform the single tree model of the previous chapter.

```{r bag, echo = FALSE, fig.cap = "\\label{fig:rmsebag}Bagging: RMSE's at Different Tree Sizes (Smoothed)", fig.align=T, fig.scap="Bagging: RMSE's at Different Tree Sizes"}
knitr::include_graphics("../00_data/output_paper/14_bagging.pdf")
```

To further analyze the bagging model, we estimated a bagging model with 25 trees for the whole dataset. Figure \ref{fig:bagvarimp} is a variable importance plot for this model. It presents those 20 variables which would increase the RMSE the most when being omitted. The plot shows that the influence of 'ms_segm' which is the market share of the wine as well as 'segmred' which is a dummy, indicating if the whine is red, substantially contribute to our model. Intuitively, the price also influences how many litres per week are sold. More surpisingly however is the fact, that also the date substantially contributes to our mode. This indicates that prices have changed oder time.

```{r bag_varimp, echo = FALSE, fig.cap = "\\label{fig:bagvarimp}Bagging: Variable Importance", fig.align=T}
knitr::include_graphics("../00_data/output_paper/15_var_imp_bagging.pdf")
```

Bagging, in general, takes all possible variables into consideration at each split. This makes bagging computationally demanding. Moreover, this may lead to similar trees at each iteration. Thus, the trees are probably highly correlated.

### Random Forest

The Random Forest algorithm is a special case of Bagging. The name random forest arises from the fact that, in cotrast to the usual Bagging algorithm, at each split of the tree building process only a pre specified number of randomly chosen variables are taken into consideration. This results in a different tree for each iteration off the random forest and therefore decorrelates the trees. In consequence, the Random Forest reduces the overall variance in comparison to Bagging. This effect is especially valid for homogenous datasets in which the trees of the bagging algorithm are highly correlated.

```{r, echo = FALSE, fig.cap= "\\label{fig:rfrmse}Random Forest: Dependency between RMSE, the Number of Trees and the Number of Variables Included at each Split", fig.scap= "RMSE's of the Random Forest for Different Parameters", fig.align= 'center'}
knitr::include_graphics("../00_data/output_paper/10_rf_plot.png")
```

We calculated RMSE values for a range of Random Forest models to evaluate which combination of the number of trees to grow and the number of variables taken into account at each split leads to the best results. The results are plotted in Figure \ref{fig:rfrmse}. The smallest RMSE is $\approx4374$. Moreover, the plot shows, reveals, that very small models profit substantially from increasing complexity while bigger models profit only by a small margin. The model with 20 Trees and 80 Variables at each split still has a RMSE of $\approx 4456$, which is an increase of less than $1.9\%$.

The 25 Tree Bagging Model (RMSE of $\approx4305$) outperforms the Random Forest Model (with 25 Trees and 100 Variables) only by a small margin (RMSE of $4374.330$). However, training the Random Forest demands only $15\%$ of the time, that the Bagging Model needs.

```{r, echo = FALSE, fig.cap= "Random Forest: Variable Importance"}
knitr::include_graphics("../00_data/output_paper/11_var_imp_random_forest_bp.pdf")
```

### Boosting

```{r boosting_hyper, echo = FALSE, fig.cap= "Boosting: Dependency between RMSE, Lambda, the Depths and the Number of Trees that are Grown", fig.scap= "RMSE's of the Boosting Model for Different Parameters", fig.align= 'center'}
knitr::include_graphics("../00_data/output_paper/11_boosting_plot.png")
```

In contrast to random forests boosting grows tree sequentially meaning that for every new tree information from already grown trees is used [@James2014, p.322]. For boosting rather small regression trees are used to improve the boosting tree slowly. This is achieved by taking the residuals from the current boosting tree, fitting a (small) tree to these residuals and then fiiting this tree into the boosting tree in areas where the performance of the boosting tree could be improved \autocite[Cf.][p.322.]{James2014}.

```{r boosting_varimp, echo = FALSE, fig.cap= "Boosting: Variable Importance Plot"}
knitr::include_graphics("../00_data/output_paper/12_var_imp_boosting_bp.pdf")
```

bag.fraction ekl채ren! Wir nutzen hier jeweils 100% unserer Observations da dies optimal ist. Man kann es aber tunen. Siehe Zeile 328 im 00er script

# Conclusion

\pagebreak

\addcontentsline{toc}{section}{References}
\printbibliography[title = References]
\cleardoublepage

\begin{refsection}
\nocite{R-base}
\nocite{R-broom}
\nocite{R-dplyr}
\nocite{R-ggplot2}
\nocite{R-haven}
\nocite{R-lmtest}
\nocite{R-PerformanceAnalytics}
\nocite{R-rstudioapi}
\nocite{R-sandwich}
\nocite{R-stargazer}
\nocite{R-svMisc}
\nocite{R-tidyr}
\nocite{R-xts}
\nocite{R-Studio}
\printbibliography[title = Software-References]
\addcontentsline{toc}{section}{Software-References}
\end{refsection}

<!---
--------------------------------------------------------------------------------
------------- Appendix ---------------------------------------------------------
--------------------------------------------------------------------------------
-->

\cleardoublepage
\appendix
\setcounter{table}{0}
\renewcommand{\thetable}{A\arabic{table}}
# Appendices

## Some Appendix 1
\newgeometry{top=1cm, left = 5cm, right = 2.5cm, bottom = 2cm}

```{r, echo = F, results = 'asis', message = F}
load("../00_data/wine_preprocessed.rda")
stargazer(as.data.frame(wine), summary.stat = c("min","max","mean","sd"),
          title = "Summary Statistics of the Data Set.", header = FALSE)
```

\restoregeometry

```{r RMSE, message = F, echo = F, include=F}
library(knitr)
library(kableExtra)

df <- data.frame(
  "Method" = c("Mean-Regression", "Linear Regression", "Forward Stepwise Selection",
               "Backward Stepwise Selection", "Regression Tree", "Pruned Tree",
               "Random Forest"),
  "RMSE" = c(0,0,0,0,0,0,0),
  "No. of Variables" = c(0,0,0,0,0,0,0)
)

kable(df, "latex")
```
